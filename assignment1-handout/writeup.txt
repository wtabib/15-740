Vittorio Perera         vdperera
Joseph Tassarotti       jtassaro
Wennie Tabib            wtabib


Problem 1:

1a) In the worst case, the measurement error will be 199.  In the best
case the measurement error is 1.  
Either it stretches the border of a tick or fits in the duration of a 
tick.  If it fits, we get a measurement of 0 so the error is 1.
If it goes over then we measure delta.  We claim it takes 10 ms which 
means it's off by a lot.


((1/(2*10^4)) - 1/(10^2))/(1/(2*10^4)) = 199

1b)  In this case the additions take half a second and there are at least 
50 deltas.  However, due to similar reasoning from (1a) itâ€™s possible that
we measure 51 deltas instead of 50.  
Therefore, 

best case = (0.5 - 50*0.01)/0.5 = 0
worst case = (0.01/0.5) = 0.02

Problem 2:

// Initializes timer, does dummy operations till it notices elapsed
// time is non zero and returns this. This is an upper bound on
// delta.

long double compute_resolution(timer_init_funct init, timer_elapsed_funct etime) {
    init();
    int n = 1;

    long double ts = etime();
    while(1) {
        long double tc = etime();
        long double diff =  tc - ts;
        if(diff > 0) {
            //printf("n = %d\n", n);
            return diff;
        }

        int a = 0;
        int j;
        for (j = 0; j < n; j++) {
            a += j;
        }
        n *= 2;
    }
}

Problem 3:

see func_time.c

Problem 4:

Our program calculated:
The clock frequency is approximately 3868 Megahertz

The output of /proc/cpuinfo is
model name  : Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz

Problem 5:

Here are the two pieces of code that we ran. 
Results follow this code.

long long thing1() {

   int i = 0;
   int numElem = 15000000;
   flushCache();
   int* arr1 = (int *) malloc(numElem*sizeof(int));
   int* arr2 = (int *) malloc(numElem*sizeof(int));
   long long numMisses = 0;

    start_cachemiss_count();
    for (i = 0; i < numElem; i++) {
        arr1[i] = i;
        arr2[i] = numElem-i;

        int elem = arr1[i];
    }

    numMisses = get_cachemiss_count();

    //free(arr1);
    //free(arr2);

    return numMisses;
}

long long thing2() {

   int i = 0;
   int numElem = 15000000;
   flushCache();
   int* arr1 = (int *) malloc(numElem*sizeof(int));
   int* arr2 = (int *) malloc(numElem*sizeof(int));
   long long numMisses = 0;

   start_cachemiss_count();

   for (i = 0; i < numElem; i++) {
        arr1[i%15] = i;
        arr2[i%15] = numElem-i;
        int elem = arr1[i%15];
   }

   numMisses = get_cachemiss_count();
   //free(arr1);
   //free(arr2);

   return numMisses;
}

The output of our program is:
total cache misses for thing1 = 2925.300049
total cache misses for thing2 = 9.200000

thing2 accesses the same small piece of memory repeatedly.  It exploits way
more locality than thing1. In contrast, thing1 accesses a much larger block of
memory and doesn't access the same element repeatedly so it has more misses as
it has to load each section of the array.   

Problem 6:

See strcat_x64.dis and strcat_naive.s

Problem 7:

Timer:
For very small strings the difference between the two is minimal.  As a string
gets very long the difference between the optimized and naive implementations
becomes larger (the optimized one is comparatively faster).

Cache:
There are very few cache misses.  
When it's also not aligned the benefit is not as great.
The thing is in memory, the cache line is huge, so there won't be many misses.  

Problem 8:
Our c code casts the string pointer to an unsigned long long pointer so that it
loads 8 bytes of the string at a time and then uses bit operations to find
where the '\0' occurs in each 8 byte chunk.  By doing this we're able to
consistently outperform the naive implementation.  On the other hand, we're
only able to beat the glibc strcat on small, aligned strings.  The problem is
that the assembly code before loading 8 bytes at a time first aligns the
address.  We keep accessing unaligned bytes.

Problem 9:

Part 1:

The first benchmark (which is in patho1.c) creates two large arrays and fills
them with random numbers.  Then, in two for loops we access the two arrays at
each iteration of the loop and sum the inputs of the arrays.  At the end of the
for loop we subtract the two sums and return the result.  The goal of this
benchmark is to stress the cache as much as possible by filling the cache
repeatedly and then accessing new elements.

The second benchmark (which is in patho2.c) creates two large arrays, fills
them with random numbers, and then instead of a sequential access (like in
patho1.c) performs a random access from the two arrays and sums the contents of
the arrays.  The random number is computed by starting with a prime number,
squaring it, and then modding by the size of the array.  At the end the two
sums are subtracted and the result is returned.  The goal of this benchmark is
to cause as many cache misses as possible by randomly accessing elements in the
array.  The misses in this benchmark should be fewer than in the first
pathological case because accessing randomly has a greater chance of hitting
than in the first case.

We compared the output of our profiling tool to pin's dcache.  The output of
our profiling tool is in patho1.txt and patho2.txt.  The output of pin's dcache
is in dcache1.out and dcache2.out.

Since dcache is only able to measure data cache misses we only compared between
our dcache misses and pin's dcache.so data cache misses. While not exactly the
same, they are very close to our results.

First case:
Ours            Pin
12476152        12471065

Second Case
Ours            Pin
5898            6082

Part 2:

The results for each of the configurations and each test can be found in the
following files:

MMM.out:
    Configuration1: MMM1.txt
    Configuration2: MMM2.txt
    Configuration3: MMM3.txt
    Configuration4: MMM4.txt

NN.out
    Configuration1: NN1.txt
    Configuration2: NN2.txt
    Configuration3: NN3.txt
    Configuration4: NN4.txt

Norm1.out
    Configuration1: Norm11.txt
    Configuration2: Norm12.txt
    Configuration3: Norm13.txt
    Configuration4: Norm14.txt

Norm2.out
    Configuration1: Norm21.txt
    Configuration2: Norm22.txt
    Configuration3: Norm23.txt
    Configuration4: Norm24.txt

In general, the number of data cache misses between configurations 3 and 4
should significantly decrease because configuration 4's cache is 4x as large as
configuration 3's cache.

MMM.out

Configuration 1 has more misses than configuration 2 which makes sense because
least recently used policy of configuration 2 allows us to better exploit
locality.  There's a slight decrease in the number of misses in Configuration 3
over Configuration 2.  The instruction cache is much less because having a
bigger line size allows us t ohave fewer instructions that go over a border.
This is particularly evident as the icache miss decreases much more than the
dcache miss.  Configuration 4 has much fewer icache misses than configuration 3
and almost the same dcache misses as configuration 3.  This is surprising
because we would expect the increased cache size to reduce the dcache misses
significantly as well.  There are a few possible reasons for this.  One is that
the capacity is still not enough (not very likely).  Another possibility is
that there are conflict misses occuring because several blocks are being mapped
to the same set which is causing collision misses.

NN.out:
There is more locality in this example than MMM.out because if we're computing
the nearest neighbor average of a pixel value at location <i,j> then at the
next iteration of the loop this value is usually a neighbor of the next pixel
<i,j+1>.  However, there are significantly more icache misses between MMM1.txt
and NN1.txt when compared to the number of executed instructions in the
program.  Configuration 2 seems to worse performance for the dcache and better
performance for the icache than configuration 1.  This is the case because in
the 2-way set associative dcache replacement policy ejects the elements that
are actually need.  a[i][j] is the first element accessed and it is also needed
at the next iteration.  Unfortunately, after all the add operations have been
performed it also the least recently used and is therefore evicted.
Configuration 3 has a huge decrease in dcache misses when compared to
configuration 2 because the line size is larger which causes fewer misses since
we're accessing sequential elements (a[i][j], a[i][j+1], a[i][j-1]).  By
similar reasoning to what happened with MMM.out we have fewer icache misses and
dcache misses.

Norm1.out:
The results of configurations 1 and 2 are similar even though configuration 2
has a 2-way set associative cache.  Apparently, for computing the froebenius
norm associativity doesn't help much.  This might be because we have very high
spatial locality because we're accessing the elements one after the other
without jumping around the array like we were in NN.out.  The dcache misses in
configuration 3 are much higher than in configuration 2 because we are doubling
the line size but the cache size stays the same. This means that, if the stores
we are making do not take more then one line in configuration 2, we're
effectively halving the cache size.  For configuration 4 the number of icache
misses significantly decreases and the number of dcache misses stays
approximately the same.

Norm2.out
Compared to Norm1, Norm2 accesses elements in column-major order.  This affects
the dcache misses because our spatial locality is much worse.  The icache is
much less affected by this difference.  The configuration 1 and 2 have almost
the same dcache miss numbers.  The icache misses for configuration 2 is
slightly lower because of the 2-way set associative caching.  Configuration 3
has almsot the same number of dcache misses again due to the poor spatial
locality.  However, there are about half as many icache misses because the line
size is 2x as large in configuration 3 as configuration 2.  Even in
configuration 4 the number of dcache misses doesn't decrease much because a
larger cache can't overcome the poor spatial locality of this program.  However
the logic of the program doesn't affect the locality of the icache.  The icache
misses decrease as the size of the cache increases.

