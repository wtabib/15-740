Problem 1:

1a) In the worst case, the measurement error will be 199.  In the best
case the measurement error is 1.  
Either it stretches the border of a tick or fits in the duration of a 
tick.  If it fits, we get a measurement of 0 so the error is 1.
If it goes over then we measure delta.  We claim it takes 10 ms which 
means it's off by a lot.


((1/(2*10^4)) - 1/(10^2))/(1/(2*10^4)) = 199

1b)  In this case the additions take half a second and there are at least 
50 deltas.  However, due to similar reasoning from (1a) itâ€™s possible that
we measure 51 deltas instead of 50.  
Therefore, 

best case = (0.5 - 50*0.01)/0.5 = 0
worst case = (0.01/0.5) = 0.02

Problem 2:

// Initializes timer, does dummy operations till it notices elapsed
// time is non zero and returns this. This is an upper bound on
// delta.

long double compute_resolution(timer_init_funct init, timer_elapsed_funct etime) {
    init();
    int n = 1;

    long double ts = etime();
    while(1) {
        long double tc = etime();
        long double diff =  tc - ts;
        if(diff > 0) {
            //printf("n = %d\n", n);
            return diff;
        }

        int a = 0;
        int j;
        for (j = 0; j < n; j++) {
            a += j;
        }
        n *= 2;
    }
}

Problem 3:

see func_time.c

Problem 4:

Our program calculated:
The clock frequency is approximately 3868 Megahertz

The output of /proc/cpuinfo is
model name  : Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz

Problem 5:

Here are the two pieces of code that we ran. 
Results follow this code.

long long thing1() {

   int i = 0;
   int numElem = 15000000;
   flushCache();
   int* arr1 = (int *) malloc(numElem*sizeof(int));
   int* arr2 = (int *) malloc(numElem*sizeof(int));
   long long numMisses = 0;

    start_cachemiss_count();
    for (i = 0; i < numElem; i++) {
        arr1[i] = i;
        arr2[i] = numElem-i;

        int elem = arr1[i];
    }

    numMisses = get_cachemiss_count();

    //free(arr1);
    //free(arr2);

    return numMisses;
}

long long thing2() {

   int i = 0;
   int numElem = 15000000;
   flushCache();
   int* arr1 = (int *) malloc(numElem*sizeof(int));
   int* arr2 = (int *) malloc(numElem*sizeof(int));
   long long numMisses = 0;

   start_cachemiss_count();

   for (i = 0; i < numElem; i++) {
        arr1[i%15] = i;
        arr2[i%15] = numElem-i;
        int elem = arr1[i%15];
   }

   numMisses = get_cachemiss_count();
   //free(arr1);
   //free(arr2);

   return numMisses;
}

The output of our program is:
total cache misses for thing1 = 2925.300049
total cache misses for thing2 = 9.200000

thing2 accesses the same small piece of memory repeatedly.  It exploits way
more locality than thing1. In contrast, thing1 accesses a much larger block of
memory and doesn't access the same element repeatedly so it has more misses as
it has to load each section of the array.   

Problem 6:

See strcat_x64.dis and strcat_naive.s

Problem 7:

Timer:
For very small strings the difference between the two is minimal.  As a string
gets very long the difference between the optimized and naive implementations
becomes larger (the optimized one is comparatively faster).

Cache:
There are very few cache misses.  
When it's also not aligned the benefit is not as great.
The thing is in memory, the cache line is huge, so there won't be many misses.  

Problem 8:
Our c code casts the string pointer to an unsigned long long pointer so that it
loads 8 bytes of the string at a time and then uses bit operations to find
where the '\0' occurs in each 8 byte chunk.  By doing this we're able to
consistently outperform the naive implementation.  On the other hand, we're
only able to beat the glibc strcat on small, aligned strings.  The problem is
that the assembly code before loading 8 bytes at a time first aligns the
address.  We keep accessing unaligned bytes.

Problem 9:

Part 1:

The first benchmark (which is in patho1.c) creates two large arrays and fills
them with random numbers.  Then, in two for loops we access the two arrays at
each iteration of the loop and sum the inputs of the arrays.  At the end of the
for loop we subtract the two sums and return the result.  The goal of this
benchmark is to stress the cache as much as possible by filling the cache
repeatedly and then accessing new elements.

The second benchmark (which is in patho2.c) creates two large arrays, fills
them with random numbers, and then instead of a sequential access (like in
patho1.c) performs a random access from the two arrays and sums the contents of
the arrays.  The random number is computed by starting with a prime number,
squaring it, and then modding by the size of the array.  At the end the two
sums are subtracted and the result is returned.  The goal of this benchmark is
to cause as many cache misses as possible by randomly accessing elements in the
array.  The misses in this benchmark should be fewer than in the first
pathological case because accessing randomly has a greater chance of hitting
than in the first case.

We compared the output of our profiling tool to pin's dcache.  The output of
our profiling tool is in patho1.txt and patho2.txt.  The output of pin's dcache
is in dcache1.out and dcache2.out.

Since dcache is only able to measure data cache misses we only compared between
our dcache misses and pin's dcache.so data cache misses. While not exactly the
same, they are very close to our results.

First case:
Ours            Pin
12476152        12471065

Second Case
Ours            Pin
5898            6082

Part 2:

